{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install cvxopt\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: The Data\n",
    "\n",
    "We will work with the data from practical 3. Load the data and split it into a training and test set. You can re-use the data splitting function from Practical 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, frac=0.3, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        \n",
    "        data = np.copy(X)\n",
    "        targets = np.copy(y)\n",
    "        #shuffling the indexes becouse both the train and test data needs to be shuffled\n",
    "        shuffled_idx = np.random.permutation(data.shape[0])\n",
    "\n",
    "        X_train = data[shuffled_idx][0:int(data.shape[0]*0.7)]\n",
    "        y_train = targets[shuffled_idx][0:int(data.shape[0]*0.7)]\n",
    "\n",
    "        X_test = data[shuffled_idx][int(data.shape[0]*0.7):]\n",
    "        y_test = targets[shuffled_idx][int(data.shape[0]*0.7):]\n",
    "        return X_train,X_test,y_train,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,mean=None,std=None):\n",
    "    if mean is None and std is None:\n",
    "\n",
    "        mean = np.mean(data,axis=0)\n",
    "        std = np.std(data,axis=0)\n",
    "    \n",
    "    else:\n",
    "        mean = mean\n",
    "        std = std\n",
    "    processed_data = (data-mean)/std\n",
    "        \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_2d, t_2d = np.load('data/nonlin_2d_data.npy')[:,:2], np.load('data/nonlin_2d_data.npy')[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, t_train, t_test = split_data(X_2d, t_2d, seed=1)\n",
    "\n",
    "train_mean = np.mean(X_train,axis=0)\n",
    "train_std = np.std(X_train,axis=0)\n",
    "\n",
    "\n",
    "norm_X_train = preprocess(X_train)\n",
    "\n",
    "norm_X_test = preprocess(X_test,mean=train_mean,std=train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 2)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Support Vector Machines\n",
    "\n",
    "First, you will implement a training algorithm for the Support Vector Machine (SVM). For solving the quadratic program, we provide a simple interface to the cvxopt library below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVMs, each data sample $x_n$ has a corresponding lagrange multiplier $\\alpha_n$ which indicates if $x_n$ is a support vector. In the latter case $\\alpha_n > 0$ holds. \n",
    "The goal of learning the SVM is to figure out which samples are support vectors by learning $\\mathbf{\\alpha}$. The dual SVM optimizes the following quadratic program.\n",
    "\n",
    "$$ \\min \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N \\alpha_n \\alpha_m t_n t_m k(\\mathbf{x}_n, \\mathbf{x}_m) - \\sum_{n=1}^N \\alpha_n$$\n",
    "subject to \n",
    "$$ 0 \\leq \\alpha_n \\leq C $$\n",
    "$$ \\sum_{n=1}^N \\alpha_n t_n = 0 $$ \n",
    "\n",
    "The quadratic program solver expects the following form:\n",
    "$$ \\min \\frac{1}{2} \\alpha^T P \\alpha + \\mathbf q^T \\mathbf \\alpha $$\n",
    "subject to \n",
    "\n",
    "$$A \\alpha = b$$\n",
    "$$G \\alpha \\leq h $$\n",
    "\n",
    "Here, $A$ and $G$ are matrices with one row per individual constraint. Similarly, $b$ and $h$ are vectors with one element per individual constraint.\n",
    "\n",
    "Having trained the SVM, a prediction for an input $\\mathbf{x}$ is made by:\n",
    "\n",
    "$$ y = sign([\\sum_n^{N} \\alpha_n t_n k(\\mathbf{x}, \\mathbf{x}_n)] + b)  $$\n",
    "\n",
    "\n",
    "### Task 1.1\n",
    " \n",
    "Use the code provided below as a basis to express the constrained optimization problem in terms of $P, q, A, b, G$ and $h$ and implement a function `fit_svm` which passes these variables to the provided QP solver. Fit a SVM on the training data and extract its parameters.\n",
    "\n",
    "**Hints:**\n",
    "  - The box constraint $0 \\leq \\alpha_n \\leq C$ defines two constraints of the form $G \\alpha_n \\leq h$ for each $\\alpha_n$.\n",
    "  - The inequality $x \\geq 0$ is equivalent to $-x \\leq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(X, t, kernel, C=1.0):\n",
    "    '''Fit SVM using data (X,t), specified kernel and parameter C.'''\n",
    "\n",
    "    t = np.array([-1 if l == 0 else 1 for l in t])\n",
    "    P = np.zeros((len(X), len(X)))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[0]):\n",
    "            k = kernel(X[i],X[j])\n",
    "            k_t = t[i]*t[j]*k\n",
    "            P[i,j] = k_t\n",
    "            \n",
    "    q = -1*np.ones((X.shape[0],1))\n",
    "    A = (t.T).reshape(-1,len(X)).astype(float)\n",
    "    b = np.zeros((1, 1))  \n",
    "    #print (b.shape[1])\n",
    "    G = np.concatenate((np.eye(X.shape[0]), -1*np.eye(X.shape[0])),axis=0)\n",
    "    h = np.concatenate((C*np.ones((X.shape[0])), np.zeros((X.shape[0]))),axis=0)\n",
    "    #h = np.concatenate((np.zeros((X.shape[0])),C*np.ones((X.shape[0]))),axis=0)\n",
    "\n",
    "    assert P.shape == (len(X), len(X))\n",
    "    assert len(q) == len(X)\n",
    "    assert A.shape == (1, len(X)) and A.dtype == 'float'\n",
    "    assert b.shape == (1, 1)\n",
    "    assert len(G) == 2 * len(X)\n",
    "    assert len(h) == 2 * len(X)\n",
    "\n",
    "    return solve_quadratic_program(P, q, A, b, G, h)\n",
    "\n",
    "def solve_quadratic_program(P, q, A, b, G, h):\n",
    "    '''Uses cvxopt to solve the quadratic program.'''\n",
    "    P, q, A, b, G, h = [cvxopt.matrix(var) for var in [P, q, A, b, G, h]]\n",
    "    minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    lagr_mult = np.ravel(minimization['x'])\n",
    "    return lagr_mult\n",
    "\n",
    "\n",
    "def extract_parameters(X, t, kernel, lagr_mult, threshold=1e-7):\n",
    "    '''Computes the intercept from the support vector constraints.\n",
    "    \n",
    "    Inputs\n",
    "        X:         predictors\n",
    "        t:         targets\n",
    "        kernel:    a kernel to be used\n",
    "        lagr_mult: the Lagrange multipliers obtained by solving the dual QP\n",
    "        threshold: threshold for choosing support vectors\n",
    "    \n",
    "    Returns\n",
    "        lagr_mult: lagrange multipliers for the support vectors\n",
    "        svs:       set of support vectors\n",
    "        sv_labels: targets t_n for the support vectors\n",
    "        intercept: computed intercept\n",
    "    '''\n",
    "    t = np.array([-1 if l == 0 else 1 for l in t])\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    \n",
    "    lagr_mult_ = lagr_mult[lagr_mult>0]\n",
    "    svs = X[lagr_mult>0]\n",
    "    sv_labels = t[lagr_mult>0]\n",
    "    weights = ((lagr_mult * t).T@X)\n",
    "    intercept = t[lagr_mult>0] - svs @weights\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return lagr_mult_, svs, sv_labels, intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1,x2,sigma=4):\n",
    "    return np.exp((-1 * np.linalg.norm(x1 - x2) ** 2) / (2 * sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7199e+01 -3.0115e+02  1e+03  2e+00  1e-15\n",
      " 1: -9.9615e+00 -1.5516e+02  2e+02  8e-02  7e-16\n",
      " 2: -1.4238e+01 -3.7492e+01  2e+01  1e-02  8e-16\n",
      " 3: -1.7975e+01 -2.6140e+01  8e+00  3e-03  7e-16\n",
      " 4: -1.9043e+01 -2.3176e+01  4e+00  1e-03  5e-16\n",
      " 5: -1.9495e+01 -2.2210e+01  3e+00  6e-04  6e-16\n",
      " 6: -2.0056e+01 -2.1189e+01  1e+00  1e-04  6e-16\n",
      " 7: -2.0361e+01 -2.0696e+01  3e-01  2e-05  6e-16\n",
      " 8: -2.0479e+01 -2.0523e+01  4e-02  2e-06  6e-16\n",
      " 9: -2.0495e+01 -2.0500e+01  5e-03  2e-08  8e-16\n",
      "10: -2.0497e+01 -2.0497e+01  2e-04  5e-10  8e-16\n",
      "11: -2.0497e+01 -2.0497e+01  3e-06  7e-12  7e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "# Fit SVM on training data\n",
    "\n",
    "lagr_mult = fit_svm(X_train,t_train,rbf_kernel)\n",
    "\n",
    "# Extract parameters\n",
    "\n",
    "lagr_mult, svs, sv_labels, intercept = extract_parameters(X_train, t_train, rbf_kernel, lagr_mult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Having learnt an SVM, we can use the calculated parameters to make predictions on novel samples.\n",
    "- Implement a function `svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept)`.\n",
    "- Use this function with your Gaussian RBF kernel (Practical 3) and compute the test accuracy on the 2d dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svm_predict(X, kernel, lagr_mult, svs, sv_labels, intercept):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    \n",
    "    kernel_matrix = np.zeros((len(X), len(svs)))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(svs.shape[0]):\n",
    "            k = kernel(X[i],svs[j])\n",
    "            kernel_matrix[i,j] = k\n",
    "    \n",
    "    print(svs.shape)\n",
    "    prediction = (sv_labels*lagr_mult) @ kernel_matrix.T  + intercept[0]\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "    \n",
    "    return np.sign(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 2)\n",
      "0.6133333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculate test accuracy\n",
    "\n",
    "#print (intercept)\n",
    "pred = svm_predict(X_test,rbf_kernel, lagr_mult, svs, sv_labels, intercept)\n",
    "pred[pred<0]=0\n",
    "print (np.sum(t_test==pred)/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "- Instead of using the Gaussian RBF kernel, use the linear kernel (dot product) defined in Practical 3.\n",
    "- Compare results on with both kernels with sklearn implementation (SVC)\n",
    "- Visualize the predictions on the test set, the learned support vectors and the decision boundary for both kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(a, b):\n",
    "    return a.T@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 2)\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "# Fit SVM with linear kernel and calculate the test accuracy\n",
    "pred_linear = svm_predict(X_test,linear_kernel, lagr_mult, svs, sv_labels, intercept)\n",
    "pred_linear[pred_linear<0]=0\n",
    "print (np.sum(t_test==pred_linear)/len(pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM using sklearn and calculate the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision Trees\n",
    "\n",
    "Next, we will implement a simple decision tree classifier using the Wine dataset, one of the standard sklearn datasets. \n",
    "\n",
    "We will use the Gini impurity as a criterion for splitting. It is defined for a set of labels as\n",
    "$$ G = \\sum_{i=0}^C p(i) * (1- p(i)) $$\n",
    "\n",
    "Given labels $l$ and split $l_a$ and $l_b$, the weighted removed impurity can be computed by $G(l) - \\frac{|l_a|}{|l|}G(l_a) - \\frac{|l_b|}{|l|}G(l_b)$.\n",
    "\n",
    "Here is a simple explanation of the Gini impurity that you may find useful: https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "1. Plot the distribution of the first feature of for each class of the wine dataset.\n",
    "2. Implement a function `gini_impurity(t)` that computes the Gini impurity for an array of labels `t`.\n",
    "3. Calculate the removed Gini impurity for a split after 50 samples, i.e. between `t[:50]` and `t[50:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n"
     ]
    }
   ],
   "source": [
    "# Load Wine dataset and split into train+test set\n",
    "\n",
    "X, t = load_wine(return_X_y=True)\n",
    "print(X.shape, t.shape)\n",
    "X_train, X_test, t_train, t_test = split_data(X, t, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gini impurity\n",
    "def gini_imp(t):\n",
    "    gini=0\n",
    "    n_classes = len(np.unique(t))\n",
    "    p = np.zeros(((n_classes),))\n",
    "    for j in range(n_classes):\n",
    "        p[j]= len(t[t==j])/(len(t))\n",
    "        \n",
    "    for c in range(n_classes):\n",
    "        gini = gini + p[c]*(1-p[c])\n",
    "    \n",
    "    return gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "Split the data along the first 12 features and plot the removed Gini impurity at different values of this feature. Which is the optimal split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "1. Implement a function `build_tree(X, t, depth)` which recursively builds a tree. Use the classes `Node` and `Leaf` as a data structure to build your tree.\n",
    "2. Implement a function `predict_tree(tree, x)` which makes a prediction for sample `x`. Obtain scores for the `wine` dataset and compare to `sklearn.tree.DecisionTree`.\n",
    "3. Switch back to the synthetic 2d dataset from the beginning (kernel methods). Compute scores and visualize the decisions in a 2d grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, n_feat, threshold):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_feat = n_feat\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_best_split(X,t):\n",
    "    gini_impurities = {}\n",
    "    for feat in range(X.shape[1]):\n",
    "        for val in X[:,feat]:\n",
    "            split_r = t[X[:,feat]>val]\n",
    "            split_l = t[X[:,feat]<=val]\n",
    "            \n",
    "            gini_r = gini_imp(split_r)\n",
    "            gini_l = gini_imp(split_l)\n",
    "            gini_total = gini_r + gini_l\n",
    "            gini_impurities[feat,val] = gini_total\n",
    "            \n",
    "    best_split = min(gini_impurities,key=gini_impurities.get)\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement recursive tree function\n",
    "\n",
    "def build_tree(X, t, depth, max_depth=3, n_labels=2):\n",
    "    \n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    \n",
    "    X_subset = X.copy()\n",
    "    t_subset = t.copy()\n",
    "    \n",
    "    best_split_feat, best_split_val  = gini_best_split(X_subset,t_subset)\n",
    "    #fixing values for the left and right nodes\n",
    "\n",
    "    X_left  = X_subset[X_subset[:,best_split_feat]<=best_split_val].copy()\n",
    "    t_left = t_subset[X_subset[:,best_split_feat]<=best_split_val].copy()\n",
    "    X_right = X_subset[X_subset[:,best_split_feat]>best_split_val].copy()\n",
    "    t_right = t_subset[X_subset[:,best_split_feat]>best_split_val].copy()\n",
    "    \n",
    "    if depth<max_depth and (len(X_left)>1 and len(X_right>1)):\n",
    "        #making the nodes\n",
    "        #level = Node(left_node, right_node, n_feat, best_split_val)\n",
    "        depth = depth+1\n",
    "        print (depth)\n",
    "        left_node = build_tree(X_left, t_left, depth, max_depth, n_labels)\n",
    "        right_node = build_tree(X_right, t_right, depth, max_depth, n_labels)\n",
    "        node = Node(left_node, right_node, best_split_feat, best_split_val)\n",
    "    else:\n",
    "        # making the leafs\n",
    "        left_label = np.argmax([np.count_nonzero(t_left==l) for l in range(n_labels)])\n",
    "        left_leaf = Leaf(left_label)\n",
    "        right_label = np.argmax([np.count_nonzero(t_right==l) for l in range(n_labels)])\n",
    "        right_leaf = Leaf(right_label)\n",
    "        node = Node(left_leaf, right_leaf, best_split_feat, best_split_val)\n",
    "        \n",
    "    return node\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    \n",
    "def predict_tree(node, x):\n",
    "    \n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "    traversed_node = node\n",
    "    while not type(traversed_node)==Leaf:\n",
    "        \n",
    "        feat_idx = traversed_node.n_feat\n",
    "        feat_threshold = traversed_node.threshold\n",
    "        if x[feat_idx]<=feat_threshold:\n",
    "            traversed_node = traversed_node.left\n",
    "        else:\n",
    "            traversed_node = traversed_node.right\n",
    "        \n",
    "    return traversed_node.label\n",
    "        \n",
    "    # ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "\n",
    "tree = build_tree(X_train, t_train, 0, max_depth=3, n_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training and test scores\n",
    "\n",
    "decision_train_pred = [predict_tree(tree,x) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(decision_train_pred==t_train)/len(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_test_pred = [predict_tree(tree,x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925925925925926\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(decision_test_pred==t_test)/len(t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score using sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test score for synthetic 2D dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
